{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from model.VAE import *\n",
    "from blackbox_pgd_model.wideresnet_update import *\n",
    "from pgd_attack import *\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from util import *\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 VAE Training')\n",
    "# parser.add_argument('--batch-size', type=int, default=200, metavar='N',\n",
    "#                     help='input batch size for training (default: 128)')\n",
    "# parser.add_argument('--test-batch-size', type=int, default=200, metavar='N',\n",
    "#                     help='input batch size for testing (default: 128)')\n",
    "# parser.add_argument('--x-dim', type=int, default=784)\n",
    "# parser.add_argument('--hidden-dim', type=int, default=400)\n",
    "# parser.add_argument('--latent-dim', type=int, default=200)\n",
    "# parser.add_argument('--epochs', type=int, default=30)\n",
    "# args = parser.parse_args()\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "batch_size = 200\n",
    "test_batch_size = 200\n",
    "beta = 0.5\n",
    "trainset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform_test)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "testset = torchvision.datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "def train(vae_model, c_model, data_loader, vae_optimizer, c_optimizer, epoch_num):\n",
    "    vae_model.train()\n",
    "    c_model.train()\n",
    "    v_loss_sum = 0\n",
    "    c_loss_sum = 0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        #data = data.view(batch_size, x_dim)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        vae_optimizer.zero_grad()\n",
    "        c_optimizer.zero_grad()\n",
    "        x_hat, mean, log_v, x_ = vae_model(data)\n",
    "        # x_cat = torch.cat((mean, log_v),1)\n",
    "        logit = c_model(x_.detach().view(-1,160,8,8))\n",
    "        #logit = c_model(x_cat)\n",
    "        v_loss, c_loss = loss_function_mean(data, target, x_hat, mean, log_v, logit)\n",
    "        #print(loss)\n",
    "        v_loss_sum += v_loss\n",
    "        c_loss_sum += c_loss\n",
    "        # if epoch_num % 2 == 1:\n",
    "        if epoch_num <= 30:\n",
    "            v_loss.backward()\n",
    "            vae_optimizer.step()\n",
    "            c_loss.backward()\n",
    "        else:\n",
    "            c_loss.backward()\n",
    "            c_optimizer.step()\n",
    "            v_loss.backward()\n",
    "    return v_loss_sum, c_loss_sum\n",
    "\n",
    "def eval_train(vae_model, c_model):\n",
    "    vae_model.eval()\n",
    "    c_model.eval()\n",
    "    err_num = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            x_hat, mean, log_v, x_ = vae_model(data)\n",
    "            logit = c_model(x_.detach().view(-1,160,8,8))\n",
    "            err_num += (logit.data.max(1)[1] != target.data).float().sum()\n",
    "    print('train error num:{}'.format(err_num))\n",
    "def eval_test(vae_model, c_model):\n",
    "    vae_model.eval()\n",
    "    c_model.eval()\n",
    "    err_num = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            x_hat, mean, log_v, x_ = vae_model(data)\n",
    "            logit = c_model(x_.detach().view(-1,160,8,8))\n",
    "            err_num += (logit.data.max(1)[1] != target.data).float().sum()\n",
    "    print('test error num:{}'.format(err_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testtime_update(vae_model, c_model, x_adv, target, learning_rate=0.1, num = 30, mode = 'mean'):\n",
    "    x_adv = x_adv.detach()\n",
    "    x_hat_adv, mean, log_v, x_ = vae_model(x_adv)\n",
    "    for _ in range(num):\n",
    "        if (x_hat_adv != x_hat_adv).sum() > 0:\n",
    "            print('nan Error')\n",
    "            exit()\n",
    "        if mode == 'mean':\n",
    "            loss = nn.functional.binary_cross_entropy(x_hat_adv, x_adv, size_average=False, reduction='mean')\n",
    "            # loss = vae_loss_mean(x_adv, x_hat_adv, mean, log_v)\n",
    "        else:\n",
    "            loss = nn.functional.binary_cross_entropy(x_hat_adv, x_adv, reduction='sum')\n",
    "            # loss = vae_loss_sum(x_adv, x_hat_adv, mean, log_v)\n",
    "        # x_.retain_grad()\n",
    "        mean.retain_grad()\n",
    "        log_v.retain_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        with torch.no_grad():\n",
    "            # x_.data -= learning_rate * x_.grad.data\n",
    "            mean.data -= learning_rate * mean.grad.data\n",
    "            log_v.data -= learning_rate * log_v.grad.data\n",
    "        # x_.grad.data.zero_()\n",
    "        mean.grad.data.zero_()\n",
    "        log_v.grad.data.zero_()\n",
    "        x_hat_adv = vae_model.decoder(vae_model.reparameterize(mean, log_v))\n",
    "        # x_hat_adv = vae_model.re_forward(x_)\n",
    "    x_cat = torch.cat((mean, log_v), 1)\n",
    "    logit_adv = c_model(x_cat)\n",
    "        # print((logit_adv.data.max(1)[1] != target.data).float().sum())\n",
    "    return logit_adv\n",
    "\n",
    "def test(vae_model, c_model):\n",
    "    err_num = 0\n",
    "    err_adv = 0\n",
    "    c_model.eval()\n",
    "    vae_model.eval()\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = Variable(data.data, requires_grad=True)\n",
    "        _,mean, log_v, x_ = vae_model(data)\n",
    "        x_cat = torch.cat((mean, log_v), 1)\n",
    "        logit = c_model(x_cat)\n",
    "        err_num += (logit.data.max(1)[1] != target.data).float().sum()\n",
    "\n",
    "        # x_adv = pgd_cifar_blackbox(vae_model, c_model, source_model, data, target, 20, 0.03, 0.003)\n",
    "        x_adv = pgd_mnist(vae_model, c_model, data, target, 40, 0.3, 0.01)\n",
    "        _,mean,log_v,x_ = vae_model(x_adv)\n",
    "        x_cat = torch.cat((mean, log_v), 1)\n",
    "        logit_adv = c_model(x_cat)\n",
    "        logit_nat_new = testtime_update(vae_model, c_model,  data, target,learning_rate=0.01, num=20)\n",
    "        logit_adv_new = testtime_update(vae_model, c_model,  x_adv, target,learning_rate=1.0, num=1)\n",
    "        return logit, logit_adv,logit_nat_new, logit_adv_new, target\n",
    "        # logit_adv = diff_update_cifar(vae_model,c_model, x_adv, target,learning_rate=0.05, num=500)\n",
    "        # _,_,_,x_adv_ = vae_model(x_adv)\n",
    "        # logit_adv = c_model(x_adv_.view(-1,160,8,8))\n",
    "        # logit = c_model(x_.view(-1,160,8,8))\n",
    "        adv_num = (logit_adv.data.max(1)[1] != target.data).float().sum()\n",
    "        # exit()\n",
    "        print(adv_num)\n",
    "        err_adv += adv_num\n",
    "        # x_cat_adv = torch.cat((m_adv, log_adv), 1)\n",
    "        # logit_adv = c_model(x_cat_adv)\n",
    "        # err_adv += (logit_adv.data.max(1)[1] != target.data).float().sum()\n",
    "    print(len(test_loader.dataset))\n",
    "    print(err_num)\n",
    "    print(err_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vae_model = VAE(zDim=256).to(device)\n",
    "c_model = classifier(input_dim=256*2).to(device)\n",
    "vae_model_path = './model-checkpoint/mnist-vae-model-54.pt'\n",
    "c_model_path = './model-checkpoint/mnist-c-model-54.pt'\n",
    "\n",
    "vae_model.load_state_dict(torch.load(vae_model_path))\n",
    "c_model.load_state_dict(torch.load(c_model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/anaconda3/envs/test/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "logit, logit_adv,logit_nat_new, logit_adv_new, target = test(vae_model, c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(176, device='cuda:0')\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
      "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0,\n",
      "        2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4,\n",
      "        1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7, 4, 4, 4, 9, 2,\n",
      "        5, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1,\n",
      "        7, 1, 8, 2, 0, 2, 9, 9, 5, 5, 1, 5, 6, 0, 3, 4, 4, 6, 5, 4, 6, 5, 4, 5,\n",
      "        1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1,\n",
      "        0, 9, 0, 3, 1, 6, 4, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# label = logit_calculate(logit_adv, logit_adv_new)\n",
    "# label = (logit_adv_new - logit_adv).data.max(1)[1]\n",
    "# label = logit_calculate(logit, logit_nat_new)\n",
    "# label = label.to(device)\n",
    "print((logit.data.max(1)[1]!=target).sum())\n",
    "# print((label!=target).sum())\n",
    "# print(label)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, device='cuda:0')\n",
      "tensor([-5.1969, 10.0791, -3.8976, -4.7219, -3.6014, -5.1634, -3.6767, -1.5791,\n",
      "        -2.7491, -4.7115], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "print(target[idx])\n",
    "print(logit[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.0447, 10.0288, -3.9240, -4.8360, -3.6265, -5.2876, -3.6403, -1.7288,\n",
      "        -2.5805, -4.4675], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logit_nat_new[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-7.2340, -4.7403, -6.8522, -4.4464, -6.8310, -5.9665, -4.9337, -8.9789,\n",
      "        17.0630, -5.8113], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([ 3.3812, -4.5472, -7.6680, -6.7861, -5.0538, -7.6097,  3.8488, -6.2713,\n",
      "        -0.3787, -3.1988], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logit_adv[idx])\n",
    "print(logit_adv_new[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00ca607cfbf8999ac685dca5ed489f11e8f7657d7eb18b4846f72ffe0c959b03"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
